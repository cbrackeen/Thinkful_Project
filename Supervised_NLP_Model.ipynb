{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from nltk.corpus import gutenberg, stopwords\n",
    "from collections import Counter\n",
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hamlet = gutenberg.raw('shakespeare-hamlet.txt')\n",
    "caesar = gutenberg.raw('shakespeare-caesar.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaner(text):\n",
    "    #Remove main header\n",
    "    text = re.sub('[\\[].*?[\\]]', '', text)\n",
    "    \n",
    "    #Remove all the Act labels\n",
    "    text = re.sub(r'Actus .[a-z]*\\.', '', text)\n",
    "    \n",
    "    #Remove all the Scene labels\n",
    "    text = re.sub(r'Scoena .[a-z]*\\.', '', text)\n",
    "    \n",
    "    #Remove any double dashes\n",
    "    text = re.sub(r'--', ' ', text)\n",
    "    \n",
    "    #Remove excess whitespace\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hamlet_clean = text_cleaner(hamlet)\n",
    "caesar_clean = text_cleaner(caesar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hamlet_doc = nlp(hamlet_clean)\n",
    "caesar_doc = nlp(caesar_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "hamlet_sent = [[sent, \"Hamlet\"] for sent in hamlet_doc.sents]\n",
    "caesar_sent = [[sent, \"Caesar\"] for sent in caesar_doc.sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(Enter, Barnardo, and, Francisco, two, Centine...</td>\n",
       "      <td>Hamlet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(Barnardo, .)</td>\n",
       "      <td>Hamlet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(Who, 's, there, ?)</td>\n",
       "      <td>Hamlet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(Fran, .)</td>\n",
       "      <td>Hamlet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(Nay, answer, me, :, Stand, &amp;, vnfold, your, s...</td>\n",
       "      <td>Hamlet</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0       1\n",
       "0  (Enter, Barnardo, and, Francisco, two, Centine...  Hamlet\n",
       "1                                      (Barnardo, .)  Hamlet\n",
       "2                                (Who, 's, there, ?)  Hamlet\n",
       "3                                          (Fran, .)  Hamlet\n",
       "4  (Nay, answer, me, :, Stand, &, vnfold, your, s...  Hamlet"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine the sentences from the two novels into one data frame.\n",
    "sentences = pd.DataFrame(hamlet_sent + caesar_sent)\n",
    "sentences.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to create a list of the 2000 most common words.\n",
    "def bag_of_words(text):\n",
    "    \n",
    "    # Filter out punctuation and stop words.\n",
    "    allwords = [token.lemma_\n",
    "                for token in text\n",
    "                if not token.is_punct\n",
    "                and not token.is_stop]\n",
    "    \n",
    "    # Return the most common words.\n",
    "    return [item[0] for item in Counter(allwords).most_common(2000)]\n",
    "    \n",
    "\n",
    "# Creates a data frame with features for each word in our common word set.\n",
    "# Each value is the count of the times the word appears in each sentence.\n",
    "def bow_features(sentences, common_words):\n",
    "    \n",
    "    # Scaffold the data frame and initialize counts to zero.\n",
    "    df = pd.DataFrame(columns=common_words)\n",
    "    df['text_sentence'] = sentences[0]\n",
    "    df['text_source'] = sentences[1]\n",
    "    df.loc[:, common_words] = 0\n",
    "    \n",
    "    # Process each row, counting the occurrence of words in each sentence.\n",
    "    for i, sentence in enumerate(df['text_sentence']):\n",
    "        \n",
    "        # Convert the sentence to lemmas, then filter out punctuation,\n",
    "        # stop words, and uncommon words.\n",
    "        words = [token.lemma_\n",
    "                 for token in sentence\n",
    "                 if (\n",
    "                     not token.is_punct\n",
    "                     and not token.is_stop\n",
    "                     and token.lemma_ in common_words\n",
    "                 )]\n",
    "        \n",
    "        # Populate the row with word counts.\n",
    "        for word in words:\n",
    "            df.loc[i, word] += 1\n",
    "        \n",
    "        # This counter is just to make sure the kernel didn't hang.\n",
    "        if i % 500 == 0:\n",
    "            print(\"Processing row {}\".format(i))\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up bags\n",
    "hamletwords = bag_of_words(hamlet_doc)\n",
    "caesarwords = bag_of_words(caesar_doc)\n",
    "\n",
    "#Combine bags to create a set of unique words.\n",
    "common_words = set(hamletwords+caesarwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 0\n",
      "Processing row 500\n",
      "Processing row 1000\n",
      "Processing row 1500\n",
      "Processing row 2000\n",
      "Processing row 2500\n",
      "Processing row 3000\n",
      "Processing row 3500\n",
      "Processing row 4000\n",
      "Processing row 4500\n",
      "Processing row 5000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fountaine</th>\n",
       "      <th>breed</th>\n",
       "      <th>i'ue</th>\n",
       "      <th>spectacle</th>\n",
       "      <th>sodaine</th>\n",
       "      <th>trophees</th>\n",
       "      <th>y</th>\n",
       "      <th>vpon</th>\n",
       "      <th>water</th>\n",
       "      <th>cressant</th>\n",
       "      <th>...</th>\n",
       "      <th>feathers</th>\n",
       "      <th>practice</th>\n",
       "      <th>deale</th>\n",
       "      <th>leane</th>\n",
       "      <th>villaines</th>\n",
       "      <th>cobl</th>\n",
       "      <th>fearefull</th>\n",
       "      <th>clothe</th>\n",
       "      <th>text_sentence</th>\n",
       "      <th>text_source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Enter, Barnardo, and, Francisco, two, Centine...</td>\n",
       "      <td>Hamlet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Barnardo, .)</td>\n",
       "      <td>Hamlet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Who, 's, there, ?)</td>\n",
       "      <td>Hamlet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Fran, .)</td>\n",
       "      <td>Hamlet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Nay, answer, me, :, Stand, &amp;, vnfold, your, s...</td>\n",
       "      <td>Hamlet</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 3138 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  fountaine breed i'ue spectacle sodaine trophees  y vpon water cressant  \\\n",
       "0         0     0    0         0       0        0  0    0     0        0   \n",
       "1         0     0    0         0       0        0  0    0     0        0   \n",
       "2         0     0    0         0       0        0  0    0     0        0   \n",
       "3         0     0    0         0       0        0  0    0     0        0   \n",
       "4         0     0    0         0       0        0  0    0     0        0   \n",
       "\n",
       "      ...     feathers practice deale leane villaines cobl fearefull clothe  \\\n",
       "0     ...            0        0     0     0         0    0         0      0   \n",
       "1     ...            0        0     0     0         0    0         0      0   \n",
       "2     ...            0        0     0     0         0    0         0      0   \n",
       "3     ...            0        0     0     0         0    0         0      0   \n",
       "4     ...            0        0     0     0         0    0         0      0   \n",
       "\n",
       "                                       text_sentence text_source  \n",
       "0  (Enter, Barnardo, and, Francisco, two, Centine...      Hamlet  \n",
       "1                                      (Barnardo, .)      Hamlet  \n",
       "2                                (Who, 's, there, ?)      Hamlet  \n",
       "3                                          (Fran, .)      Hamlet  \n",
       "4  (Nay, answer, me, :, Stand, &, vnfold, your, s...      Hamlet  \n",
       "\n",
       "[5 rows x 3138 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create our data frame with features.\n",
    "word_counts = bow_features(sentences, common_words)\n",
    "word_counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts.to_csv('SNM_words.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stopword Counter\n",
    "def stopword_count(sent):\n",
    "    count = 0\n",
    "    for word in sent:\n",
    "        if str(word) in stopwords.words('english'):\n",
    "            count+=1\n",
    "    return count\n",
    "\n",
    "#Add stopword count feature\n",
    "def add_stopword_ct(sents, dataframe):\n",
    "    counts = [stopword_count(token) for token in sents[0]]\n",
    "    counter = pd.DataFrame(counts)\n",
    "    counter.columns = ['stopwords']\n",
    "    new_df = pd.concat((dataframe, counter), axis=1)\n",
    "    return new_df\n",
    "\n",
    "#Add sentence length feature\n",
    "def add_sent_leng(sents, dataframe):\n",
    "    lens = [len(token) for token in sents[0]]\n",
    "    lengths = pd.DataFrame(lens)\n",
    "    lengths.columns = ['length']\n",
    "    new_df = pd.concat((dataframe, lengths), axis=1)\n",
    "    return new_df\n",
    "\n",
    "#Punctuation counter\n",
    "def punct_count(token):\n",
    "    count=0\n",
    "    for word in token:\n",
    "        if word.is_punct:\n",
    "            count+=1\n",
    "    return count\n",
    "\n",
    "#Add punctuation count feature\n",
    "def add_punc_count(sents, dataframe):\n",
    "    counts = [punct_count(token) for token in sents[0]]\n",
    "    punctuation = pd.DataFrame(counts)\n",
    "    punctuation.columns = ['punctuation']\n",
    "    new_df = pd.concat((dataframe, punctuation), axis=1)\n",
    "    return new_df\n",
    "\n",
    "#Unique word counter\n",
    "def unique_wd(sent):\n",
    "    words = [word for word in sent if not word.is_punct]\n",
    "    u_words = set([token.text for token in words])\n",
    "    return len(u_words)\n",
    "        \n",
    "#Add unique words feature\n",
    "def add_unique_words(sents, dataframe):\n",
    "    words = [unique_wd(token) for token in sents[0]]\n",
    "    word_ct = pd.DataFrame(words)\n",
    "    word_ct.columns = ['unique']\n",
    "    new_df = pd.concat((dataframe, word_ct), axis=1)\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw_counts = add_stopword_ct(sentences, word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw_len = add_sent_leng(sentences, sw_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw_punc = add_punc_count(sentences, sw_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw_unique = add_unique_words(sentences, sw_punc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing Bag of Words\n",
    "X = word_counts.drop(['text_sentence','text_source'],1)\n",
    "Y = word_counts['text_source']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,Y, test_size=0.3, random_state=1334)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6426313471474762\n"
     ]
    }
   ],
   "source": [
    "rfc = ensemble.RandomForestClassifier(n_estimators=10, max_depth=5)\n",
    "rfc.fit(X_train,y_train)\n",
    "rfc_bow = cross_val_score(rfc, X_test, y_test, cv=10)\n",
    "print(rfc_bow.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing Handmade Features\n",
    "X_hm = sw_unique.loc[:,['stopwords','length','punctuation','unique']]\n",
    "Y_hm = word_counts['text_source']\n",
    "Xhm_train, Xhm_test, yhm_train, yhm_test = train_test_split(X_hm,Y_hm,test_size=0.3, \n",
    "                                                            random_state=1334)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5841119095957806\n"
     ]
    }
   ],
   "source": [
    "rfc_hm = ensemble.RandomForestClassifier(n_estimators=10, max_depth=5)\n",
    "rfc_hm.fit(Xhm_train,yhm_train)\n",
    "rfc_hm_bow = cross_val_score(rfc_hm, Xhm_test, yhm_test, cv=10)\n",
    "print(rfc_hm_bow.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing Tf-idf\n",
    "\n",
    "hamletp=gutenberg.paras('shakespeare-hamlet.txt')\n",
    "caesarp=gutenberg.paras('shakespeare-caesar.txt')\n",
    "\n",
    "hamlet_para=[]\n",
    "caesar_para=[]\n",
    "\n",
    "for paragraph in hamletp:\n",
    "    para=paragraph[0]\n",
    "    #removing the double-dash from all words\n",
    "    para=[re.sub(r'--','',word) for word in para]\n",
    "    #Forming each paragraph into a string and adding it to the list of strings.\n",
    "    hamlet_para.append([' '.join(para), 'Hamlet'])\n",
    "    \n",
    "for paragraph in caesarp:\n",
    "    para=paragraph[0]\n",
    "    #removing the double-dash from all words\n",
    "    para=[re.sub(r'--','',word) for word in para]\n",
    "    #Forming each paragraph into a string and adding it to the list of strings.\n",
    "    caesar_para.append([' '.join(para), 'Caesar'])\n",
    "                        \n",
    "paragraphs = pd.DataFrame(hamlet_para+caesar_para)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_df=0.5, # drop words that occur in more than half the paragraphs\n",
    "                             min_df=2, # only use words that appear at least twice\n",
    "                             stop_words='english', \n",
    "                             lowercase=True, #convert everything to lower case (since Alice in Wonderland has the HABIT of CAPITALIZING WORDS for EMPHASIS)\n",
    "                             use_idf=True,#we definitely want to use inverse document frequencies in our weighting\n",
    "                             norm=u'l2', #Applies a correction factor so that longer paragraphs and shorter paragraphs get treated equally\n",
    "                             smooth_idf=True #Adds 1 to all document frequencies, as if an extra document existed that used every word once.  Prevents divide-by-zero errors\n",
    "                            )\n",
    "sents_tfidf = vectorizer.fit_transform(paragraphs.iloc[:,0])\n",
    "\n",
    "X_train_tfidf, X_test_tfidf, y_train_tfidf, y_test_tfidf = train_test_split(\n",
    "    sents_tfidf, paragraphs.iloc[:,1], test_size=0.3,random_state=1334)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_tfidf = ensemble.RandomForestClassifier(n_estimators=10, max_depth=5)\n",
    "rfc_tfidf.fit(X_train_tfidf, y_train_tfidf)\n",
    "rfc_tfidf_score = cross_val_score(rfc_tfidf, X_test_tfidf,y_test_tfidf, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7736772247360482"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc_tfidf_score.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
